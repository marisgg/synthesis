{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "from config import *\n",
    "import operator\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plt.rcParams.update({\n",
    "# #     \"pgf.texsystem\": \"lualatex\"\n",
    "# # })\n",
    "font_scale = 2\n",
    "plt.rcParams['pgf.texsystem'] = 'pdflatex'\n",
    "sns.set_theme(style=\"whitegrid\",font_scale=font_scale)\n",
    "# fontsize = 35\n",
    "# plt.rc(\"font\", **{\"family\": \"serif\", \"serif\": [\"Times\"]})#, \"size\" : fontsize})\n",
    "# plt.rc(\"text\", usetex=True)\n",
    "\n",
    "# plt.rcParams['text.latex.preamble'] = [r'\\usepackage{amsmath}'] #for \\text command\n",
    "plt.rcParams[\"pgf.preamble\"] = r'\\usepackage{amsmath}'\n",
    "plt.rcParams.update({\"text.latex.preamble\": plt.rcParams[\"pgf.preamble\"] })\n",
    "plt.rcParams['text.usetex'] = True\n",
    "save_format = 'pgf'\n",
    "backend = 'pgf'\n",
    "transparent = True\n",
    "pad_inches = 0.1\n",
    "bbox_inches = 'tight'\n",
    "save_args = {\n",
    "    'format' : save_format,\n",
    "    'backend' : backend,\n",
    "    'transparent' : transparent,\n",
    "    'pad_inches' : pad_inches,\n",
    "    'bbox_inches' : bbox_inches,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR=BASE_OUTPUT_DIR\n",
    "SUBFAMILY_SIZE=10\n",
    "SEED=2\n",
    "SEEDS=list(range(2,12))\n",
    "ENV_NAMES = ['dpm', 'obstacles-10-2', 'avoid', 'obstacles-8-3', 'rover', 'network']\n",
    "MINIMIZING = [False, True, True, True, False, False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEEDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If statistics.pickle does not exist, then execute the code under 'gather statistics' first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./statistics.pickle\", 'rb') as handle:\n",
    "    statistics = pickle.load(handle)\n",
    "statistics = {key.split(\"/\")[-1] : value for key, value in statistics.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_TO_MINIMIZING = {env : mini for env, mini in zip(ENV_NAMES, MINIMIZING)}\n",
    "\n",
    "def get_env_str(env : str):\n",
    "    if True:\n",
    "        prefix = '$\\\\downarrow$~' if ENV_TO_MINIMIZING[env] else '$\\\\uparrow$~'\n",
    "        suffix = ''\n",
    "        # suffix = '}'\n",
    "        # prefix = '\\\\emph{'\n",
    "        if env == 'dpm':\n",
    "            env_id = prefix + f\"{env.upper()}\" + suffix\n",
    "        elif '-' in env:\n",
    "            env_id = prefix + f\"{env.split(\"-\")[0][0].upper()}{env.split(\"-\")[0][1:].lower()}({env.split(\"-\")[1]}, {env.split(\"-\")[2]})\" + suffix\n",
    "            env_id = env_id.replace(\"3\", \"5\") # Obstacles(8,3) is Obstacles(8,5)\n",
    "        else:\n",
    "            env_id = prefix + f\"{env[0].upper()}{env[1:].lower()}\" + suffix\n",
    "    return env_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap(x):\n",
    "    return f'${x}$'\n",
    "\n",
    "dimensions = {}\n",
    "for env in ENV_NAMES:\n",
    "    dimensions[get_env_str(env)] = {\n",
    "        '$|\\\\family|$' : wrap(statistics[env]['family_size']),\n",
    "                '$|S|$' : wrap(statistics[env]['max_num_states']),\n",
    "        '$|\\\\obsset|$' : wrap(statistics[env]['num_observations']), \n",
    "        '$|\\\\Act|$' : wrap(statistics[env]['num_actions']),\n",
    "\n",
    "        \n",
    "        }\n",
    "    print(f\"nS:{statistics[env]['max_num_states']}\\tnO:{statistics[env]['num_observations']}\\t\\tnA:{statistics[env]['num_actions']}\", env, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def significant(x, p):\n",
    "    x_positive = np.where(np.isfinite(x) & (x != 0), np.abs(x), 10**(p-1))\n",
    "    mags = 10 ** (p - 1 - np.floor(np.log10(x_positive)))\n",
    "    return np.round(x * mags) / mags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.DataFrame(dimensions).T.style.to_latex(hrules=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"./plot_builder_output-pgf\"\n",
    "os.makedirs(f\"{OUTPUT_DIR}/heatmaps\", exist_ok=True)\n",
    "os.makedirs(f\"{OUTPUT_DIR}/lineplot\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# heatmap func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def make_heatmap(results : dict, our_results : dict, env : str, title : str, baseline : str, union_results : dict = None, minimizing = True, add_whole_family_value = True):\n",
    "    print(title)\n",
    "    values = np.array([np.ravel(list(item[-2].values())) for train, item in results.items()])#.squeeze(axis=-1)\n",
    "    values[~np.isfinite(values)] = np.nan\n",
    "    if union_results:\n",
    "        if type(union_results['subfamily']) == dict:\n",
    "            ur = list(union_results['subfamily'].values())\n",
    "        else:\n",
    "            ur = union_results['subfamily']\n",
    "            \n",
    "        values = np.vstack([values, ur])\n",
    "    values = np.vstack([values, our_results['ours']])\n",
    "    if add_whole_family_value:\n",
    "        entire_family_values = np.array([item[-1] if item[-1] is not None else np.nan for train, item in results.items()] + ([union_results['whole_family']] if union_results else []) + [our_results['whole_family']])\n",
    "        # entire_family_values[~np.isfinite(entire_family_values)] = np.nan\n",
    "        values = np.hstack([values, entire_family_values[..., None]])\n",
    "    # plt.figure(figsize=(16,9), dpi=300)\n",
    "    \n",
    "    if minimizing:\n",
    "        colormap = sns.cm.rocket_r\n",
    "        # colormap.set_bad(sns.cm.rocket.get_bad())\n",
    "    else:\n",
    "        colormap = sns.cm.rocket\n",
    "    # colormap.set_bad('black')\n",
    "    \n",
    "    plt.figure(figsize=(16,9))\n",
    "    \n",
    "    ax = sns.heatmap(values, annot=True, cbar=False, vmin=np.nanmin(values), vmax=np.nanmax(values), cmap=colormap, fmt='.4g')#, annot_kws={\"size\": 25})#, mask=~np.isfinite(values))\n",
    "    \n",
    "    # if add_whole_family_value:\n",
    "        # ax.add_patch(Rectangle((0,0), 10, 11, fill=False, edgecolor='white', lw=3)) # White bounding box\n",
    "    \n",
    "    subfamily_size = len(results.keys())\n",
    "    \n",
    "    best_value = np.inf if minimizing else -np.inf\n",
    "    for r in range(values.shape[0]):\n",
    "        row_values = values[r][:-1] if add_whole_family_value else values[r]\n",
    "        if np.isfinite(row_values).any():\n",
    "            idx = np.nanargmax(row_values) if minimizing else np.nanargmin(row_values)\n",
    "        else:\n",
    "            idx = random.randint(0, len(row_values)-1)\n",
    "        cmp = operator.le if minimizing else operator.ge\n",
    "        if cmp(row_values[idx], best_value):\n",
    "            best_value = row_values[idx]\n",
    "            best_rectangle = (idx, r)\n",
    "        ax.add_patch(Rectangle((idx, r),1,1, fill=False, edgecolor='blue', lw=3))\n",
    "    ax.add_patch(Rectangle(best_rectangle,1,1, fill=False, edgecolor='green', lw=3))\n",
    "    \n",
    "    if add_whole_family_value:\n",
    "        best_family_idx = np.nanargmin(values[:, -1]) if minimizing else np.nanargmax(values[:, -1])\n",
    "        ax.add_patch(Rectangle((subfamily_size, best_family_idx),1,1, fill=False, edgecolor='green', lw=3))\n",
    "    \n",
    "    # ax.set_xlabel(\"Test\")\n",
    "    # ax.set_ylabel(\"Train\")\n",
    "    plt.yticks(rotation=0)\n",
    "    xticks = [\"$M_{\" + f\"{i+1}\" + \"}$\" for i in range(subfamily_size)]\n",
    "    if add_whole_family_value:\n",
    "        xticks += [\"$\\mathcal{M}$\"]\n",
    "    ax.set_xticklabels(xticks)#, rotation=45, ha='right', rotation_mode='anchor')\n",
    "    # ax.set_xticklabels(xticks)\n",
    "    suffix = lambda i : f\" ({results[i][1].num_nodes if results[i][1] else None}-FSC)\"\n",
    "    suffix = lambda _ : \"\"\n",
    "    prefixE = r'\\textsc{gd-E}' if 'gd' in baseline.lower() else r'\\textsc{Saynt-E}' if 'saynt' in baseline.lower() else 'WHAT?'\n",
    "    prefixU = r'\\textsc{gd-U}' if 'gd' in baseline.lower() else r'\\textsc{Saynt-U}' if 'saynt' in baseline.lower() else 'WHAT?'\n",
    "    ax.set_yticklabels([\"$M_{\" + f\"{i+1}\" + \"}$\" for i in range(subfamily_size)] + ([prefixU] if union_results else []) + [r'\\textsc{rfPG-S}']) # [\"rfPG on subfamily\"])\n",
    "    # ax.set_yticklabels([prefixE + f\" on POMDP {i+1}{suffix(i)}\" for i in range(subfamily_size)] + ([prefixU + f\" on Union POMDP\"] if union_results else []) + [r'\\textsc{rfPG-S} on Subfamily']) # [\"rfPG on subfamily\"])\n",
    "    # ax.set_title(f\"{title}: {baseline} baselines vs \" + r'\\textsc{rfPG-S}' + f\" for a single run ({'lower' if minimizing else 'higher'} is better)\")\n",
    "    ax.set_title(title)\n",
    "    plt.tight_layout()\n",
    "    print(env)\n",
    "    plt.savefig(f\"{OUTPUT_DIR}/heatmaps/{env}-{baseline}.{save_format}\", **save_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "\n",
    "def make_lineplot(results : dict, title : str = 'Placeholder', minimizing = True, type_of_plot = 'family_trace', use_time_x_axis=False):\n",
    "    fig = plt.figure(dpi=300)\n",
    "    ax = fig.gca()\n",
    "    plt.title(f\"{title} ({'lower' if minimizing else 'higher'} is better)\")\n",
    "    plt.ylabel(\"Worst family member value\")\n",
    "    \n",
    "    max_time_normal = max(results['gd-normal']['plot_times'])\n",
    "    max_time_random = max(results['gd-random']['plot_times'])\n",
    "    \n",
    "    results['gd-random']['plot_times'] = (np.array(results['gd-random']['plot_times']) / max_time_random) * max_time_normal\n",
    "    \n",
    "    print(max_time_normal, max_time_random)\n",
    "    \n",
    "    if use_time_x_axis:\n",
    "        plt.xlabel(\"Time\")\n",
    "        print(results['gd-normal']['plot_times'])\n",
    "        ax.plot(results['gd-normal']['plot_times'], results['gd-normal'][type_of_plot], label='rfPG', color='green')\n",
    "    else:\n",
    "        plt.xlabel(\"Iteration\")\n",
    "        ax.plot(results['gd-normal'][type_of_plot], label='rfPG', color='green')\n",
    "    \n",
    "    min_x_normal = np.argmin(results['gd-normal'][type_of_plot]) if minimizing else np.argmax(results['gd-normal'][type_of_plot])\n",
    "    min_y_normal = results['gd-normal'][type_of_plot][min_x_normal]\n",
    "    if use_time_x_axis: min_x_normal = results['gd-normal']['plot_times'][min_x_normal]\n",
    "    \n",
    "    # plt.plot(results['gd-no-momentum'][type_of_plot], label='rfPG (no momentum)', color='blue')\n",
    "    \n",
    "    # min_x_no_mom = np.argmin(results['gd-no-momentum'][type_of_plot]) if minimizing else np.argmax(results['gd-no-momentum'][type_of_plot])\n",
    "    # min_y_no_mom = results['gd-no-momentum'][type_of_plot][min_x_no_mom]\n",
    "    \n",
    "    \n",
    "    \n",
    "    if use_time_x_axis:\n",
    "        ax.plot(results['gd-random']['plot_times'], results['gd-random'][type_of_plot], label='Random rfPG', color='red')\n",
    "    else:\n",
    "        exit()\n",
    "        ax.plot(results['gd-random'][type_of_plot], label='Random rfPG', color='red')\n",
    "\n",
    "    min_x = np.argmin(results['gd-random'][type_of_plot]) if minimizing else np.argmax(results['gd-random'][type_of_plot])\n",
    "    min_y = results['gd-random'][type_of_plot][min_x]\n",
    "    if use_time_x_axis: min_x = results['gd-random']['plot_times'][min_x]\n",
    "    \n",
    "    mini = min([min_x, min_x_normal])\n",
    "    maxi = max([min_x, min_x_normal])\n",
    "    \n",
    "    def get(x):\n",
    "        if math.isclose(mini, maxi) and math.isclose(x, maxi):\n",
    "            return random.randint(-50, 50)\n",
    "        elif math.isclose(x, maxi):\n",
    "            return 50\n",
    "        elif math.isclose(x, mini):\n",
    "            return -50\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    ax.annotate(f\"{min_y:.2f}\",\n",
    "            xy=(min_x, min_y), xycoords='data',\n",
    "            xytext=(get(min_x), (50 if minimizing else -50)), textcoords='offset points',\n",
    "            arrowprops=dict(facecolor='red', shrink=0),\n",
    "            horizontalalignment='center', verticalalignment='bottom')\n",
    "    \n",
    "    ax.annotate(f\"{min_y_normal:.2f}\",\n",
    "                xy=(min_x_normal, min_y_normal), xycoords='data',\n",
    "                xytext=(get(min_x_normal), (50 if minimizing else -50)), textcoords='offset points',\n",
    "                arrowprops=dict(facecolor='green', shrink=0),\n",
    "                horizontalalignment='center', verticalalignment='bottom')\n",
    "    \n",
    "    # ax.annotate(f\"{min_y_no_mom:.2f}\",\n",
    "    #     xy=(min_x_no_mom, min_y_no_mom), xycoords='data',\n",
    "    #     xytext=(get(min_x_no_mom), (50 if minimizing else -50)), textcoords='offset points',\n",
    "    #     arrowprops=dict(facecolor='blue', shrink=0),\n",
    "    #     horizontalalignment='center', verticalalignment='bottom')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{OUTPUT_DIR}/lineplot/{title}.{save_format}\", **save_args)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve_single_seed(envs=ENV_NAMES, minimizings=MINIMIZING, seed=SEED):\n",
    "    for env, minimizing in zip(envs, minimizings):\n",
    "        try:\n",
    "            with open(f\"{BASE_DIR}/{env}/{seed}/gd-experiment.pickle\", 'rb') as handle:\n",
    "                results = pickle.load(handle)\n",
    "                make_lineplot(results, title=env, minimizing=minimizing, use_time_x_axis=True)\n",
    "        except FileNotFoundError as fnfe:\n",
    "            print(fnfe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "\n",
    "def plot_learning_curve(envs=ENV_NAMES, minimizings=MINIMIZING, seed=SEED, use_ci=True):\n",
    "    \n",
    "    x_common = np.linspace(0, 100, 1000)\n",
    "    \n",
    "    for env, minimizing in zip(envs, minimizings):\n",
    "        rand_X = []\n",
    "        rand_Y = []\n",
    "        X = []\n",
    "        Y = []\n",
    "        for seed in SEEDS:\n",
    "            try:\n",
    "                with open(f\"{BASE_DIR}/{env}/seed{seed}/gd-experiment.pickle\", 'rb') as handle:\n",
    "                    results = pickle.load(handle)\n",
    "                    # make_lineplot(results, title=env, minimizing=minimizing, use_time_x_axis=True)\n",
    "            except FileNotFoundError as fnfe:\n",
    "                print(fnfe)\n",
    "                continue\n",
    "            \n",
    "            # print(env, seed)\n",
    "            type_of_plot = 'family_trace'                    \n",
    "            \n",
    "            x_random = np.array(results['gd-random']['plot_times']) \n",
    "            y_random = np.array(results['gd-random'][type_of_plot])\n",
    "            \n",
    "            rand_X.append(x_random)\n",
    "            rand_Y.append(y_random)\n",
    "\n",
    "            x = np.array(results['gd-normal']['plot_times'])\n",
    "            y = np.array(results['gd-normal'][type_of_plot])\n",
    "            \n",
    "            # if True:\n",
    "            try:\n",
    "                max_time_normal = max(results['gd-normal']['plot_times'])\n",
    "                max_time_random = max(results['gd-random']['plot_times'])\n",
    "            except:\n",
    "                continue\n",
    "            \n",
    "            if max_time_random >= max_time_normal:\n",
    "                x_random = (x_random / max_time_random) * max_time_normal\n",
    "            else:\n",
    "                x = (x / max_time_random) * max_time_normal\n",
    "            \n",
    "            X.append(x)\n",
    "            Y.append(y)\n",
    "            \n",
    "            # y_common = np.interp(x_common, this_x, this_y)\n",
    "            \n",
    "            # print(max_time_normal, max_time_random)\n",
    "        \n",
    "        num_points = 360\n",
    "        x_common = np.linspace(0, 3600, num_points)\n",
    "\n",
    "        \n",
    "        def plot_interp(X, Y):\n",
    "            # y_sum = np.zeros(num_points)\n",
    "            y_agg = []\n",
    "            \n",
    "            assert len(rand_X) == len(rand_Y)\n",
    "            number_of_curves = len(rand_X)\n",
    "\n",
    "            for this_x, this_y in zip(X, Y):                \n",
    "                # Interpolate y so that it's using a common x-axis\n",
    "                try:\n",
    "                    y_common = np.interp(x_common, this_x, this_y)\n",
    "                except Exception as e:\n",
    "                    # print(this_x, this_y)\n",
    "                    continue\n",
    "                \n",
    "                # Add it to the other curves\n",
    "                y_agg += [y_common]\n",
    "\n",
    "            # Divide the sum by the number of curves to get the average curve\n",
    "            # y_average = y_sum / number_of_curves\n",
    "            \n",
    "            return y_agg\n",
    "            \n",
    "        # fig = plt.figure(figsize=(4,3), dpi=100)\n",
    "        # fig = plt.figure(figsize=(1.755 * 2,1.31 * 2))\n",
    "        # plt.figure(dpi=100)\n",
    "        # plt.grid(True)\n",
    "        # ax = fig.gca()\n",
    "        rand_Y_interp = plot_interp(rand_X, rand_Y)\n",
    "        # sns.lineplot(x=rand_X_interp, y=rand_Y_interp, label='Random rfPG', color='red')\n",
    "        plt.plot(x_common, np.mean(rand_Y_interp, axis=0), label='Random', color='red')\n",
    "        \n",
    "        if use_ci:\n",
    "            ci_a, ci_b = scipy.stats.t.interval(0.95, len(x_common)-1, loc=np.mean(rand_Y_interp, axis=0), scale=scipy.stats.sem(rand_Y_interp, axis=0))\n",
    "            plt.fill_between(x_common, ci_a, ci_b,  alpha=0.25, color='red')\n",
    "        else:\n",
    "            plt.fill_between(x_common, np.mean(rand_Y_interp, axis=0) + np.var(rand_Y_interp, axis=0), np.mean(rand_Y_interp, axis=0) - np.var(rand_Y_interp, axis=0),  alpha=0.25, color='red')\n",
    "        \n",
    "        Y_interp = plot_interp(X, Y)\n",
    "        plt.plot(x_common, np.mean(Y_interp, axis=0), label=r'\\textsc{rfPG}', color='green')\n",
    "\n",
    "        if use_ci:\n",
    "            ci_a, ci_b = scipy.stats.t.interval(0.95, len(x_common)-1, loc=np.mean(Y_interp, axis=0), scale=scipy.stats.sem(Y_interp, axis=0))\n",
    "            plt.fill_between(x_common, ci_a, ci_b,  alpha=0.25, color='green')\n",
    "        else:\n",
    "            plt.fill_between(x_common, np.mean(Y_interp, axis=0) + np.std(Y_interp, axis=0), np.mean(Y_interp, axis=0) - np.std(Y_interp, axis=0),  alpha=0.25, color='green')\n",
    "        # plt.title(f\"{env.upper()} ({'lower' if minimizing else 'higher'} is better)\")\n",
    "        plt.title(get_env_str(env))\n",
    "        plt.ylabel(\"Robust performance\")\n",
    "        plt.xlabel(\"Time (seconds)\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        os.makedirs(f\"{OUTPUT_DIR}/lineplot\", exist_ok=True)\n",
    "        # plt.savefig(f\"{OUTPUT_DIR}/lineplot/{env}.{save_format}\", **save_args)\n",
    "        plt.savefig(f\"{OUTPUT_DIR}/lineplot/{env}.{save_format}\", backend='pgf', format='pgf', bbox_inches = 'tight', pad_inches = 0.05)\n",
    "        plt.show()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEFAULT FUNC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_results(env : str, include_union_results : bool = True, seed=SEED, subfamsize=SUBFAMILY_SIZE, include_gd_results=False, include_saynt_results : bool = True):\n",
    "    if include_saynt_results:\n",
    "        with open(f\"{BASE_DIR}/{env}/subfamsize{subfamsize}/seed{seed}/subfam-saynt.pickle\", 'rb') as handle:\n",
    "            saynt = pickle.load(handle)\n",
    "    else:\n",
    "        saynt = None\n",
    "        \n",
    "    with open(f\"{BASE_DIR}/{env}/subfamsize{subfamsize}/seed{seed}/subfam-ours.pickle\", 'rb') as handle:\n",
    "        ours = pickle.load(handle)\n",
    "    \n",
    "    if include_gd_results:\n",
    "        with open(f\"{BASE_DIR}/{env}/subfamsize{subfamsize}/seed{seed}/subfam-gradient.pickle\", 'rb') as handle:\n",
    "            gradient = pickle.load(handle)\n",
    "    else:\n",
    "        gradient = None\n",
    "    \n",
    "    if include_union_results:\n",
    "        union_results = {}\n",
    "        try:\n",
    "            with open(f\"{BASE_DIR}/{env}/union/seed{seed}/union-saynt.pickle\", 'rb') as handle: \n",
    "                union_results['saynt'] = pickle.load(handle)\n",
    "            if type(union_results['saynt']['subfamily']) == dict:\n",
    "                union_results['saynt']['subfamily'] = list(union_results['saynt']['subfamily'].values())\n",
    "        except FileNotFoundError as e:\n",
    "            print(e)\n",
    "            union_results['saynt'] = None\n",
    "        try:\n",
    "            with open(f\"{BASE_DIR}/{env}/union/seed{seed}/union-gradient.pickle\", 'rb') as handle: \n",
    "                union_results['gradient'] = pickle.load(handle)\n",
    "        except FileNotFoundError as e:\n",
    "            print(e)\n",
    "            union_results['gradient'] = None\n",
    "    else:\n",
    "        union_results = None\n",
    "    \n",
    "    return saynt, ours, union_results, gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table(envs : list[str], minimizings : list[bool], include_union_results = True):\n",
    "    subfamresults = {}\n",
    "    whlfamresults = {}\n",
    "    for env, minimizing in sorted(zip(envs, minimizings), key=lambda x : x[1]):\n",
    "        saynt, ours, union_results, gradient = load_results(env, include_union_results=include_union_results, include_gd_results=True)\n",
    "        \n",
    "        aggregator = lambda x : float((max if minimizing else min)(x))        \n",
    "        \n",
    "        env_id = f\"{env.upper()} ({'min.' if minimizing else 'max.'})\"\n",
    "        \n",
    "        \n",
    "        subfamresults[env_id] = {\n",
    "            # min if minimizing because taking the best FSC out of the 10 FSCs, but worst value for that FSC among the 10 POMDPs evaluated. \n",
    "            '\\\\saynt (one-by-one)' : (min if minimizing else max)([aggregator(np.array(list(saynt[k][2].values())).flatten().tolist()) for k in saynt.keys()]),\n",
    "            '\\\\saynt (union)' : aggregator(union_results['subfamily']) if union_results else np.nan,\n",
    "            '\\\\ours  (subfamily)'  : aggregator(ours['ours']),\n",
    "        }\n",
    "        \n",
    "        with open(f\"{BASE_DIR}/{env}/seed{SEED}/gd-experiment.pickle\", 'rb') as handle:\n",
    "            rfpg_results = pickle.load(handle)\n",
    "            \n",
    "        \n",
    "        \n",
    "        whlfamresults[env_id] = {\n",
    "            '\\\\saynt (one-by-one)' : aggregator([item[-1] if item[-1] is not None else np.nan for train, item in saynt.items()]),\n",
    "            '\\\\saynt (union)' : float(union_results['whole_family']) if union_results else np.nan,\n",
    "            '\\\\ours  (subfamily)'  : float(ours['whole_family']),\n",
    "            '\\\\ours  (whole family)' : rfpg_results['gd-normal']['best_worst_value']\n",
    "        }\n",
    "    \n",
    "    return subfamresults, whlfamresults\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def hotfix(x : np.ndarray, set_inf_to_random_family_value = False, env = None) -> np.ndarray:\n",
    "    # x = x[~np.isnan(x)]\n",
    "    # if set_inf_to_random_family_value:\n",
    "        # x[~np.isfinite(x)] = statistics[env]['family_value']\n",
    "    return x\n",
    "\n",
    "# one_by_one_suffix = ' (enum.)'\n",
    "# one_by_one_suffix = '-E'\n",
    "one_by_one_suffix = 'E'\n",
    "# union_suffix = ' (union)'\n",
    "# union_suffix = '-U'\n",
    "union_suffix = 'U'\n",
    "\n",
    "def create_table_multiple_seeds(envs : list[str], minimizings : list[bool], include_union_results = True, include_gd_results = True, normalize_results = True, include_error = False, seeds=SEEDS):\n",
    "    subfamresults = defaultdict(dict)\n",
    "    whlfamresults = defaultdict(dict)\n",
    "    for env, minimizing in sorted(zip(envs, minimizings), key=lambda x : x[1]):\n",
    "        \n",
    "        aggregator = lambda x : float((max if minimizing else min)(x))        \n",
    "        \n",
    "\n",
    "        env_id = get_env_str(env)       \n",
    "        \n",
    "        subfamresults[env_id] = defaultdict(list)\n",
    "        whlfamresults[env_id] = defaultdict(list)\n",
    "        \n",
    "        # nan_num = np.inf if minimizing \n",
    "        \n",
    "\n",
    "\n",
    "        count = 0\n",
    "        \n",
    "        for seed in seeds:\n",
    "            try:\n",
    "                # saynt, ours, union_results = load_results(env, include_union_results=include_union_results, seed=seed)\n",
    "                saynt, ours, union_results, gradient = load_results(env, include_union_results=include_union_results, seed=seed, include_gd_results=include_gd_results)\n",
    "                \n",
    "                with open(f\"{BASE_DIR}/{env}/seed{seed}/gd-experiment.pickle\", 'rb') as handle:\n",
    "                    rfpg_results = pickle.load(handle)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                continue\n",
    "\n",
    "            count += 1\n",
    "            \n",
    "            whl_fam_results_norm = rfpg_results['gd-normal']['best_worst_value']\n",
    "            assert np.isfinite(whl_fam_results_norm)\n",
    "            assert whl_fam_results_norm > 0\n",
    "            \n",
    "            sub_fam_results_norm = aggregator(ours['ours'])\n",
    "            assert np.isfinite(sub_fam_results_norm)\n",
    "            assert sub_fam_results_norm > 0\n",
    "            \n",
    "            norm_sub = lambda x : (sub_fam_results_norm / x if minimizing else x / sub_fam_results_norm) if normalize_results else x\n",
    "            norm_whl = lambda x : (whl_fam_results_norm / x if minimizing else x / whl_fam_results_norm) if normalize_results else x\n",
    "            \n",
    "            random_policy_subfam = statistics[env][seed]['evaluations'].max() if minimizing else statistics[env][seed]['evaluations'].min() # get random policy value of worst POMDP inside subfamily\n",
    "            random_policy_whlfam = statistics[env]['family_value']\n",
    "            \n",
    "            if minimizing:\n",
    "                assert random_policy_subfam <= random_policy_whlfam, (random_policy_subfam, random_policy_whlfam)\n",
    "            else:\n",
    "                assert random_policy_subfam >= random_policy_whlfam, (random_policy_subfam, random_policy_whlfam)\n",
    "            \n",
    "            # print(gradient)\n",
    "            # subfamresults[env_id]['Direction'] = 'min.' if minimizing else 'max.'\n",
    "            subfamresults[env_id][f'\\\\saynt{one_by_one_suffix}'] += [norm_sub((min if minimizing else max)([aggregator(np.array(list(saynt[k][2].values())).flatten().tolist()) for k in saynt.keys()]))]\n",
    "            if union_results['saynt']:\n",
    "                res = aggregator(union_results['saynt']['subfamily'])\n",
    "                if not np.isfinite(res):\n",
    "                    # print(env, seed, \"NOT FINITE:\", res)\n",
    "                    res = random_policy_subfam\n",
    "                subfamresults[env_id][f'\\\\saynt{union_suffix}'] += [norm_sub(res)]\n",
    "            \n",
    "            if include_gd_results:\n",
    "                subfamresults[env_id][f'\\\\gd{one_by_one_suffix}'] += [norm_sub((min if minimizing else max)([aggregator(np.array(list(gradient[k][2].values())).flatten().tolist()) for k in gradient.keys()]))]\n",
    "                if union_results['gradient']:\n",
    "                    res = aggregator(union_results['gradient']['subfamily'])\n",
    "                    if not np.isfinite(res):\n",
    "                        # print(env, seed, \"NOT FINITE:\", res)\n",
    "                        res = random_policy_subfam\n",
    "                    subfamresults[env_id][f'\\\\gd{union_suffix}'] += [norm_sub(res)]\n",
    "            \n",
    "            subfamresults[env_id]['\\\\ours  (subfamily)'] += [norm_sub(aggregator(ours['ours']))]            \n",
    "            \n",
    "            # whlfamresults[env_id]['Random'] = statistics[env]\n",
    "            # whlfamresults[env_id]['Direction'] = 'min.' if minimizing else 'max.'\n",
    "            # whlfamresults[env_id]['$|\\\\family|$'] = statistics[env]['family_size']\n",
    "            # whlfamresults[env_id]['Random'] = statistics[env]['family_value']\n",
    "            # print(env, seed, '\\\\saynt (one-by-one)', [item[-1] for train, item in saynt.items()])\n",
    "            whlfamresults[env_id][f'\\\\saynt{one_by_one_suffix}'] += [norm_whl(aggregator([item[-1] if item[-1] is not None else random_policy_whlfam for train, item in saynt.items()]))]\n",
    "            if union_results['saynt']:\n",
    "                res = float(union_results['saynt']['whole_family'])\n",
    "                if not np.isfinite(res):\n",
    "                    res = statistics[env]['family_value']\n",
    "                    assert np.isfinite(res), res\n",
    "                whlfamresults[env_id][f'\\\\saynt{union_suffix}'] += [norm_whl(res)]\n",
    "            # print(whlfamresults[env_id]['\\\\saynt (one-by-one)'])\n",
    "                \n",
    "            \n",
    "            if include_gd_results:\n",
    "                whlfamresults[env_id][f'\\\\gd{one_by_one_suffix}'] += [norm_whl(aggregator([item[-1] if item[-1] is not None else random_policy_whlfam for train, item in gradient.items()]))]\n",
    "                if union_results['gradient']:\n",
    "                    whlfamresults[env_id][f'\\\\gd{union_suffix}'] += [norm_whl(float(union_results['gradient']['whole_family']))]\n",
    "            \n",
    "            whlfamresults[env_id]['\\\\ours  (subfamily)'] += [norm_whl(float(ours['whole_family']))]\n",
    "            whlfamresults[env_id]['\\\\ours  (whole family)'] += [rfpg_results['gd-normal']['best_worst_value']]\n",
    "        \n",
    "        # if 'avoid' in env.lower():\n",
    "            # print(env_id, subfamresults[env_id]['\\\\saynt (union)'])\n",
    "        \n",
    "        # print(env_id, whlfamresults[env_id])\n",
    "        \n",
    "        assert count > 0, (env)\n",
    "        # print(env, whlfamresults[env_id]['\\\\saynt (one-by-one)'])\n",
    "        \n",
    "        # statistic_func = lambda x : np.round(np.mean(x), decimals=2)\n",
    "        # statistic_func = lambda x : np.format_float_scientific(np.mean(x),precision=2,trim='k')\n",
    "        # statistic_func = lambda x : f\"${np.format_float_scientific(np.mean(x),precision=2,trim='k')} \\pm {np.format_float_scientific(np.std(x),precision=2,trim='k')}$\"\n",
    "        # statistic_func = lambda x : f\"${np.mean(x):.4g}\\pm {(scipy.stats.sem(x) if scipy.stats.sem(x) > 0.001 else 0.0):.2g}$\"\n",
    "        whl_fam_results_norm = rfpg_results['gd-normal']['best_worst_value']\n",
    "        assert np.isfinite(whl_fam_results_norm)\n",
    "        assert whl_fam_results_norm > 0\n",
    "        statistic_func_whl = lambda x : f\"${np.mean(whl_fam_results_norm / np.array(x)if minimizing else np.array(x) / whl_fam_results_norm):.4g}$\"\n",
    "        \n",
    "        sub_fam_results_norm = aggregator(ours['ours'])\n",
    "        assert np.isfinite(sub_fam_results_norm)\n",
    "        assert sub_fam_results_norm > 0\n",
    "        statistic_func_sub = lambda x : f\"${np.mean(sub_fam_results_norm / np.array(x) if minimizing else np.array(x) / sub_fam_results_norm):.4g}$\"\n",
    "        # statistic_func = lambda x : f\"${np.mean(x):.4g}\\pm {np.std(x):.4g}$\"\n",
    "        # statistic_func = lambda x : f\"${np.mean(x):.4g}\\pm {scipy.stats.t.interval(0.95, len(x)-1, loc=np.mean(x), scale=scipy.stats.sem(x))[0]:.4g}$\"\n",
    "        # statistic_func = lambda x : f\"${np.round(np.mean(x), decimals=2)} \\pm {np.round(np.std(x), decimals=2)}$\"\n",
    "        \n",
    "        # print(env_id, subfamresults[env_id][f'\\\\saynt ({one_by_one})'])\n",
    "        \n",
    "        print('sub norm:', sub_fam_results_norm)\n",
    "        print('whl norm:', whl_fam_results_norm)\n",
    "        \n",
    "        # for key, value in subfamresults.items():\n",
    "        #     subfamresults[key] = statistic_func(value)\n",
    "        \n",
    "        for d, _ in zip([subfamresults, whlfamresults], [statistic_func_sub, statistic_func_whl]):\n",
    "            if include_error:\n",
    "                statistic_func = lambda x : f\"${np.mean(x):.2f} \\pm {scipy.stats.sem(x):.2f}$\" if normalize_results else f\"${np.mean(x):.4g} \\pm {scipy.stats.sem(x):.4g}$\"\n",
    "            else:\n",
    "                statistic_func = lambda x : f\"${np.mean(x):.2f}$\" if normalize_results else f\"${np.mean(x):.4g}$\"\n",
    "            bests = []\n",
    "            best_number = np.inf if minimizing else -np.inf\n",
    "            for key, value in d[env_id].items():\n",
    "                d[env_id][key] = statistic_func(value)\n",
    "                average = np.mean(value)\n",
    "                # average = float(d[env_id][key].replace(\"$\", \"\"))\n",
    "                if minimizing and average <= best_number:\n",
    "                    if math.isclose(average, best_number):\n",
    "                        bests.append(key)\n",
    "                    else:\n",
    "                        best_number = average\n",
    "                        bests = [key]\n",
    "                if not minimizing and average >= best_number:\n",
    "                    if math.isclose(average, best_number):\n",
    "                        bests.append(key)\n",
    "                    else:\n",
    "                        best_number = average\n",
    "                        bests = [key]\n",
    "                    \n",
    "                \n",
    "            for best in bests:\n",
    "                print(best)\n",
    "\n",
    "    return subfamresults, whlfamresults\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEEDS[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subfam, whlfam = create_table(ENV_NAMES, MINIMIZING, include_union_results=True)\n",
    "_, whlfam = create_table_multiple_seeds(ENV_NAMES, MINIMIZING, include_union_results=True, normalize_results=False, include_error=False, seeds=SEEDS[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subfam, whlfam = create_table(ENV_NAMES, MINIMIZING, include_union_results=True)\n",
    "subfam, whlfam = create_table_multiple_seeds(ENV_NAMES, MINIMIZING, include_union_results=True, normalize_results=True, include_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pd_highlight() -> str:\n",
    "    return \"highlight:--rwrap;\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_latex_str_new(df : pd.DataFrame, highlight=False):\n",
    "    # df = df.round(decimals=2)\n",
    "    return df.style.format(precision=2).to_latex(hrules=True, multicol_align='l')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_latex_str(df : pd.DataFrame):\n",
    "    df = df.round(decimals=2)\n",
    "    return df[:3].style.highlight_max(axis=1, props=pd_highlight().format(precision=2)).concat(df[3:].style.highlight_min(axis=1, props=pd_highlight()).format(precision=2)).format(precision=2).to_latex(hrules=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Whole family results:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(whlfam).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(whlfam).T.iloc[:, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(subfam).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(whlfam).T.iloc[:, :-1]\n",
    "last_col = pd.DataFrame(whlfam).T.iloc[:, -1]\n",
    "df2 = pd.DataFrame(subfam).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df1.join(df2, lsuffix=\"_whl\", rsuffix=\"_sub\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_col = [col + suffix for pair in zip(df1.columns, df2.columns) for col, suffix in zip(pair, ['_sub', '_whl'])]\n",
    "new_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(new_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = [\n",
    "f'\\\\saynt{one_by_one_suffix}',\n",
    "f'\\\\saynt{union_suffix}',\n",
    "  f'\\\\gd{one_by_one_suffix}',\n",
    " f'\\\\gd{union_suffix}',\n",
    " '\\\\ours']\n",
    "\n",
    "def create_df_for_table_one(df1, df2, last_col):\n",
    "    df = df1.join(df2, lsuffix=\"_whl\", rsuffix=\"_sub\")\n",
    "    new_col = [col + suffix for pair in zip(df1.columns, df2.columns) for col, suffix in zip(pair, ['_sub', '_whl'])]\n",
    "    df = df[new_col]\n",
    "    df = pd.concat([df, last_col], axis=1)\n",
    "    # methods = list(set(list(subfam[ENV_NAMES[0].upper()].keys()) + list(whlfam[ENV_NAMES[0].upper()].keys())))\n",
    "    idx = pd.MultiIndex(levels=[methods, ['Subset', 'Whole'], ['Subset', 'Whole']],\n",
    "                codes=[[0, 0, 1, 1, 2, 2, 3, 3, 4, 4, 4], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1]],\n",
    "                names=['Method', 'Synthesis', 'Evaluation'])\n",
    "    df.columns = idx\n",
    "    return df\n",
    "\n",
    "def create_df_for_table_two(df1, df2, last_col):\n",
    "    df = df1.join(df2, lsuffix=\"_whl\", rsuffix=\"_sub\")\n",
    "    print(df.columns)\n",
    "    # new_col = [col + suffix for pair in zip(df1.columns, df2.columns) for col, suffix in zip(pair, ['_sub', '_whl'])]\n",
    "    # df = df[new_col]\n",
    "    df = pd.concat([df, last_col], axis=1)\n",
    "    idx = pd.MultiIndex(levels=[['Subset', 'Whole'], ['Subset', 'Whole'], methods],\n",
    "                codes=[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1], [0, 0, 1, 1, 2, 2, 3, 3, 4, 4, 4]],\n",
    "                names=['Synthesis', 'Evaluation', 'Method'])\n",
    "    df.columns = idx\n",
    "    return df\n",
    "\n",
    "def create_df_for_table_three(df1, df2, last_col, flip_synth_eval=False):\n",
    "    df = df1.join(df2, lsuffix=\"_whl\", rsuffix=\"_sub\")\n",
    "    print(df.columns)\n",
    "    new_col = [\n",
    "        f'\\\\saynt{one_by_one_suffix}',\n",
    "f'\\\\saynt{union_suffix}',\n",
    "  f'\\\\gd{one_by_one_suffix}',\n",
    " f'\\\\gd{union_suffix}',\n",
    "    ]\n",
    "    new_col = [f\"{m}_sub\" for m in methods if not 'ours' in m.lower()] + ['\\\\ours  (subfamily)_sub'] + [f\"{m}_whl\" for m in methods if not 'ours' in m.lower()] + ['\\\\ours  (subfamily)_whl']\n",
    "    # new_col = [col + suffix for pair in zip(df1.columns, df2.columns) for col, suffix in zip(pair, ['_sub', '_whl'])]\n",
    "    df = df[new_col]\n",
    "    df = pd.concat([df, last_col], axis=1)\n",
    "    codes = [[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 4]] if flip_synth_eval else [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1], [0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 4]]\n",
    "    names = ['Evaluation', 'Synthesis', 'Method'] if flip_synth_eval else ['Synthesis', 'Evaluation', 'Method']\n",
    "    idx = pd.MultiIndex(levels=[['Subset', 'Whole'], ['Subset', 'Whole'], methods],\n",
    "                codes=codes,\n",
    "                names=names)\n",
    "    df.columns = idx\n",
    "    return df\n",
    "\n",
    "def create_df_for_table_four(df1, df2, last_col):\n",
    "    df = df1.join(df2, lsuffix=\"_whl\", rsuffix=\"_sub\")\n",
    "    print(df.columns)\n",
    "    new_col = [\n",
    "        f'\\\\saynt{one_by_one_suffix}',\n",
    "f'\\\\saynt{union_suffix}',\n",
    "  f'\\\\gd{one_by_one_suffix}',\n",
    " f'\\\\gd{union_suffix}',\n",
    "    ]\n",
    "    new_col = [f\"{m}_sub\" for m in methods if not 'ours' in m.lower()] + ['\\\\ours  (subfamily)_sub'] + [f\"{m}_whl\" for m in methods if not 'ours' in m.lower()] + ['\\\\ours  (subfamily)_whl']\n",
    "    # new_col = [col + suffix for pair in zip(df1.columns, df2.columns) for col, suffix in zip(pair, ['_sub', '_whl'])]\n",
    "    df = df[new_col]\n",
    "    df = pd.concat([df, last_col], axis=1)\n",
    "    idx = pd.MultiIndex(levels=[['Subset', 'Whole'], ['Subset', 'Whole'], methods, ['Enum', 'Union', '']],\n",
    "                codes=[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1], [0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 4], [0, 1, 0, 1, 2, 0, 1, 0, 1, 2, 2]],\n",
    "                names=['Synthesis', 'Evaluation', 'Method', 'Type'])\n",
    "    df.columns = idx\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = create_df_for_table_one(df1, df2, last_col)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_df_for_table_two(df1, df2, last_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{OUTPUT_DIR}/joint.tex\", 'w') as file:\n",
    "    print(to_latex_str_new(df), file=file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = create_df_for_table_three(df1, df2, last_col)\n",
    "pd.concat([df.iloc[:, 0:4], df.iloc[:, 5:]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = create_df_for_table_four(df1, df2, last_col)\n",
    "df = pd.concat([df.iloc[:, 0:4], df.iloc[:, 5:-1]], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{OUTPUT_DIR}/joint.tex\", 'w') as file:\n",
    "    print(to_latex_str_new(df), file=file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subdf = df.iloc[:, :4]\n",
    "with open(f\"{OUTPUT_DIR}/joint-error-1.tex\", 'w') as file:\n",
    "    print(to_latex_str_new(subdf), file=file)\n",
    "subdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subdf = df.iloc[:, 5:]\n",
    "with open(f\"{OUTPUT_DIR}/joint-error-2.tex\", 'w') as file:\n",
    "    print(to_latex_str_new(subdf), file=file)\n",
    "subdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\".join([\"\\\\cmidrule(lr){\" + f\"{i}-{i}\" + \"}\" for i in range(1, 13)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = create_df_for_table_four(df1, df2, last_col)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:3, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = df[:3].style.highlight_max(axis=1, props=pd_highlight().format(precision=2)).concat(df[3:].style.highlight_min(axis=1, props=pd_highlight()).format(precision=2)).format(precision=2).to_latex(hrules=True)\n",
    "print(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = list(set(list(subfam[ENV_NAMES[0].upper()].keys()) + list(whlfam[ENV_NAMES[0].upper()].keys())))\n",
    "methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    " '\\\\saynt (enum.)_sub',\n",
    " '\\\\saynt (enum.)_whl',\n",
    " '\\\\saynt (union)_sub',\n",
    " '\\\\saynt (union)_whl',\n",
    " '\\\\gd (enum.)_sub',\n",
    " '\\\\gd (enum.)_whl',\n",
    " '\\\\gd (union)_sub',\n",
    " '\\\\gd (union)_whl',\n",
    " '\\\\ours  (subfamily)_sub',\n",
    " '\\\\ours  (subfamily)_whl'\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(to_latex_str_new(pd.DataFrame(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(to_latex_str(pd.DataFrame(whlfam).T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = [\n",
    " '\\\\saynt (enum.)',\n",
    " '\\\\saynt (union)',\n",
    " '\\\\gd (enum.)',\n",
    " '\\\\gd (union)',\n",
    " '\\\\ours'\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{OUTPUT_DIR}/joint.tex\", 'w') as file:\n",
    "    print(to_latex_str_new(df), file=file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{OUTPUT_DIR}/whlfam.tex\", 'w') as file:\n",
    "    print(to_latex_str(pd.DataFrame(whlfam).T), file=file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{OUTPUT_DIR}/subfam.tex\", 'w') as file:\n",
    "    print(to_latex_str(pd.DataFrame(subfam).T), file=file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(subfam).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(to_latex_str(pd.DataFrame(subfam).T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_heatmap(env : str, plot_gradient_baseline = False, include_union_results = False, seed=SEED, subfamsize=SUBFAMILY_SIZE, **kwargs):\n",
    "    saynt, ours, union_results, gradient = load_results(env, include_union_results=include_union_results, include_gd_results=plot_gradient_baseline, seed=seed, subfamsize=subfamsize)\n",
    "    # print(union_results_saynt)\n",
    "    # print(union_results_gradient)\n",
    "    if plot_gradient_baseline:\n",
    "        with open(f\"{BASE_DIR}/{env}/subfamsize{subfamsize}/seed{seed}/subfam-gradient.pickle\", 'rb') as handle:\n",
    "            gradient = pickle.load(handle)\n",
    "\n",
    "        if gradient and union_results['gradient']:\n",
    "            make_heatmap(gradient, ours, env, get_env_str(env), \"GD\", union_results=union_results['gradient'], **kwargs)\n",
    "        else:\n",
    "            print(\"Gradient Union results None for\", env, seed)\n",
    "\n",
    "    if union_results and union_results['saynt']:\n",
    "        make_heatmap(saynt, ours, env, get_env_str(env), \"Saynt\", union_results=union_results['saynt'], **kwargs)\n",
    "    else:\n",
    "        print(\"Saynt Union results None for\", env, seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heatmaps ALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = 'obstacles-illustrative'\n",
    "sns.set_theme(style=\"whitegrid\",font_scale=3)\n",
    "\n",
    "saynt, ours, union_results, gradient = load_results(env, include_union_results=False, include_gd_results=True, include_saynt_results=False, seed=11, subfamsize=3)\n",
    "\n",
    "make_heatmap(gradient, ours, env, r\"$\\downarrow~$Illustrative Example\", \"GD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = 'obstacles-illustrative'\n",
    "sns.set_theme(style=\"whitegrid\",font_scale=4)\n",
    "\n",
    "saynt, ours, union_results, gradient = load_results(env, include_union_results=False, include_gd_results=True, include_saynt_results=False, seed=11, subfamsize=3)\n",
    "ours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = 'obstacles-illustrative'\n",
    "sns.set_theme(style=\"whitegrid\",font_scale=4)\n",
    "\n",
    "temp = BASE_DIR\n",
    "BASE_DIR = './example-test/parallel-IJCAI-example-deterministic'\n",
    "\n",
    "saynt, ours, union_results, gradient = load_results(env, include_union_results=False, include_gd_results=True, include_saynt_results=False, seed=11, subfamsize=3)\n",
    "add_whole_family_value = True\n",
    "minimizing = True\n",
    "results = gradient\n",
    "our_results = ours\n",
    "values = np.array([np.ravel(list(item[-2].values())) for train, item in results.items()])#.squeeze(axis=-1)\n",
    "values[~np.isfinite(values)] = np.nan\n",
    "# if add_whole_family_value:\n",
    "    # entire_family_values = np.array([item[-1] for train, item in results.items()])\n",
    "    # values = np.hstack([values, entire_family_values[..., None]])\n",
    "if union_results:\n",
    "    # print(values, union_results['subfamily'])\n",
    "    # print(np.shape(values), np.shape()\n",
    "    if type(union_results['saynt']['subfamily']) == dict:\n",
    "        ur = list(union_results['saynt']['subfamily'].values())\n",
    "    else:\n",
    "        ur = union_results['saynt']['subfamily']\n",
    "        \n",
    "    values = np.vstack([values, ur])\n",
    "    \n",
    "    if type(union_results['gradient']['subfamily']) == dict:\n",
    "        ur = list(union_results['gradient']['subfamily'].values())\n",
    "    else:\n",
    "        ur = union_results['gradient']['subfamily']\n",
    "        \n",
    "    values = np.vstack([values, ur])\n",
    "values = np.vstack([values, our_results['ours']])\n",
    "if add_whole_family_value:\n",
    "    entire_family_values = np.array([item[-1] if item[-1] is not None else np.nan for train, item in results.items()] + ([union_results['saynt']['whole_family'], union_results['gradient']['whole_family']] if union_results else []) + [our_results['whole_family']])\n",
    "    # entire_family_values[~np.isfinite(entire_family_values)] = np.nan\n",
    "    values = np.hstack([values, entire_family_values[..., None]])\n",
    "# plt.figure(figsize=(16,9), dpi=300)\n",
    "\n",
    "if minimizing:\n",
    "    colormap = sns.cm.rocket_r\n",
    "    # colormap.set_bad(sns.cm.rocket.get_bad())\n",
    "else:\n",
    "    colormap = sns.cm.rocket\n",
    "# colormap.set_bad('black')\n",
    "\n",
    "plt.figure(figsize=(16,9))\n",
    "\n",
    "ax = sns.heatmap(values, annot=True, cbar=False, vmin=np.nanmin(values), vmax=np.nanmax(values), cmap=colormap, fmt='.0f')#, annot_kws={\"size\": 25})#, mask=~np.isfinite(values))\n",
    "\n",
    "# if add_whole_family_value:\n",
    "    # ax.add_patch(Rectangle((0,0), 10, 11, fill=False, edgecolor='white', lw=3)) # White bounding box\n",
    "\n",
    "subfamily_size = len(results.keys())\n",
    "\n",
    "best_value = np.inf if minimizing else -np.inf\n",
    "for r in range(values.shape[0]):\n",
    "    row_values = values[r][:-1] if add_whole_family_value else values[r]\n",
    "    if np.isfinite(row_values).any():\n",
    "        idx = np.nanargmax(row_values) if minimizing else np.nanargmin(row_values)\n",
    "    else:\n",
    "        idx = random.randint(0, len(row_values)-1)\n",
    "    cmp = operator.le if minimizing else operator.ge\n",
    "    if cmp(row_values[idx], best_value):\n",
    "        best_value = row_values[idx]\n",
    "        best_rectangle = (idx, r)\n",
    "    ax.add_patch(Rectangle((idx, r),1,1, fill=False, edgecolor='blue', lw=3))\n",
    "ax.add_patch(Rectangle(best_rectangle,1,1, fill=False, edgecolor='green', lw=3))\n",
    "\n",
    "if add_whole_family_value:\n",
    "    best_family_idx = np.nanargmin(values[:, -1]) if minimizing else np.nanargmax(values[:, -1])\n",
    "    ax.add_patch(Rectangle((subfamily_size, best_family_idx),1,1, fill=False, edgecolor='green', lw=3))\n",
    "\n",
    "# ax.set_xlabel(\"Test\")\n",
    "# ax.set_ylabel(\"Train\")\n",
    "plt.yticks(rotation=0)\n",
    "xticks = [\"$M_{\" + f\"{i+1}\" + \"}$\" for i in range(subfamily_size)]\n",
    "if add_whole_family_value:\n",
    "    xticks += [\"$\\mathcal{M}$\"]\n",
    "ax.set_xticklabels(xticks)#, rotation=45, ha='right', rotation_mode='anchor')\n",
    "# ax.set_xticklabels(xticks)\n",
    "suffix = lambda i : f\" ({results[i][1].num_nodes if results[i][1] else None}-FSC)\"\n",
    "suffix = lambda _ : \"\"\n",
    "# prefixE = r'\\textsc{gd-E}' if 'gd' in baseline.lower() else r'\\textsc{Saynt-E}' if 'saynt' in baseline.lower() else 'WHAT?'\n",
    "# prefixU = r'\\textsc{gd-U}' if 'gd' in baseline.lower() else r'\\textsc{Saynt-U}' if 'saynt' in baseline.lower() else 'WHAT?'\n",
    "ax.set_yticklabels([\"$\\\\displaystyle M_{\" + f\"{i+1}\" + \"}$\" for i in range(subfamily_size)] + (['\\\\textsc{gd-U}', '\\\\textsc{Saynt-U}'] if union_results else []) + ['\\\\textsc{rfPG}']) # [\"rfPG on subfamily\"])\n",
    "# ax.set_yticklabels([prefixE + f\" on POMDP {i+1}{suffix(i)}\" for i in range(subfamily_size)] + ([prefixU + f\" on Union POMDP\"] if union_results else []) + [r'\\textsc{rfPG-S} on Subfamily']) # [\"rfPG on subfamily\"])\n",
    "# ax.set_title(f\"{title}: {baseline} baselines vs \" + r'\\textsc{rfPG-S}' + f\" for a single run ({'lower' if minimizing else 'higher'} is better)\")\n",
    "# ax.set_title(get_env_str(env))\n",
    "# plt.tight_layout()\n",
    "print(env)\n",
    "plt.savefig(f\"{OUTPUT_DIR}/heatmaps/{env}-main-body\", **save_args)\n",
    "save_args2 = {k : v for k, v in save_args.items()}\n",
    "save_args2['backend'] = 'pdf'\n",
    "save_args2['format']  = 'pdf'\n",
    "print(save_args, save_args2)\n",
    "plt.savefig(f\"{OUTPUT_DIR}/heatmaps/{env}-main-body.pdf\", **save_args2)\n",
    "plt.show()\n",
    "\n",
    "BASE_DIR = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = 'obstacles-8-3'\n",
    "sns.set_theme(style=\"whitegrid\",font_scale=2)\n",
    "\n",
    "saynt, ours, union_results, gradient = load_results(env, include_union_results=True, include_gd_results=True, include_saynt_results=True, seed=SEEDS[0], subfamsize=10)\n",
    "add_whole_family_value = True\n",
    "minimizing = True\n",
    "results = saynt\n",
    "our_results = ours\n",
    "values = np.array([np.ravel(list(item[-2].values())) for train, item in results.items()])#.squeeze(axis=-1)\n",
    "values[~np.isfinite(values)] = np.nan\n",
    "# if add_whole_family_value:\n",
    "    # entire_family_values = np.array([item[-1] for train, item in results.items()])\n",
    "    # values = np.hstack([values, entire_family_values[..., None]])\n",
    "if union_results:\n",
    "    # print(values, union_results['subfamily'])\n",
    "    # print(np.shape(values), np.shape()\n",
    "    if type(union_results['saynt']['subfamily']) == dict:\n",
    "        ur = list(union_results['saynt']['subfamily'].values())\n",
    "    else:\n",
    "        ur = union_results['saynt']['subfamily']\n",
    "        \n",
    "    values = np.vstack([values, ur])\n",
    "    \n",
    "    if type(union_results['gradient']['subfamily']) == dict:\n",
    "        ur = list(union_results['gradient']['subfamily'].values())\n",
    "    else:\n",
    "        ur = union_results['gradient']['subfamily']\n",
    "        \n",
    "    values = np.vstack([values, ur])\n",
    "values = np.vstack([values, our_results['ours']])\n",
    "if add_whole_family_value:\n",
    "    entire_family_values = np.array([item[-1] if item[-1] is not None else np.nan for train, item in results.items()] + ([union_results['saynt']['whole_family'], union_results['gradient']['whole_family']] if union_results else []) + [our_results['whole_family']])\n",
    "    # entire_family_values[~np.isfinite(entire_family_values)] = np.nan\n",
    "    values = np.hstack([values, entire_family_values[..., None]])\n",
    "# plt.figure(figsize=(16,9), dpi=300)\n",
    "\n",
    "if minimizing:\n",
    "    colormap = sns.cm.rocket_r\n",
    "    # colormap.set_bad(sns.cm.rocket.get_bad())\n",
    "else:\n",
    "    colormap = sns.cm.rocket\n",
    "# colormap.set_bad('black')\n",
    "\n",
    "plt.figure(figsize=(16,9))\n",
    "\n",
    "ax = sns.heatmap(values, annot=True, cbar=False, vmin=np.nanmin(values), vmax=np.nanmax(values), cmap=colormap, fmt='.0f')#, annot_kws={\"size\": 25})#, mask=~np.isfinite(values))\n",
    "\n",
    "# if add_whole_family_value:\n",
    "    # ax.add_patch(Rectangle((0,0), 10, 11, fill=False, edgecolor='white', lw=3)) # White bounding box\n",
    "\n",
    "subfamily_size = len(results.keys())\n",
    "\n",
    "best_value = np.inf if minimizing else -np.inf\n",
    "for r in range(values.shape[0]):\n",
    "    row_values = values[r][:-1] if add_whole_family_value else values[r]\n",
    "    if np.isfinite(row_values).any():\n",
    "        idx = np.nanargmax(row_values) if minimizing else np.nanargmin(row_values)\n",
    "    else:\n",
    "        idx = random.randint(0, len(row_values)-1)\n",
    "    cmp = operator.le if minimizing else operator.ge\n",
    "    if cmp(row_values[idx], best_value):\n",
    "        best_value = row_values[idx]\n",
    "        best_rectangle = (idx, r)\n",
    "    ax.add_patch(Rectangle((idx, r),1,1, fill=False, edgecolor='blue', lw=3))\n",
    "ax.add_patch(Rectangle(best_rectangle,1,1, fill=False, edgecolor='green', lw=3))\n",
    "\n",
    "if add_whole_family_value:\n",
    "    best_family_idx = np.nanargmin(values[:, -1]) if minimizing else np.nanargmax(values[:, -1])\n",
    "    ax.add_patch(Rectangle((subfamily_size, best_family_idx),1,1, fill=False, edgecolor='green', lw=3))\n",
    "\n",
    "# ax.set_xlabel(\"Test\")\n",
    "# ax.set_ylabel(\"Train\")\n",
    "plt.yticks(rotation=0)\n",
    "xticks = [\"$M_{\" + f\"{i+1}\" + \"}$\" for i in range(subfamily_size)]\n",
    "if add_whole_family_value:\n",
    "    xticks += [\"$\\mathcal{M}$\"]\n",
    "ax.set_xticklabels(xticks)#, rotation=45, ha='right', rotation_mode='anchor')\n",
    "# ax.set_xticklabels(xticks)\n",
    "suffix = lambda i : f\" ({results[i][1].num_nodes if results[i][1] else None}-FSC)\"\n",
    "suffix = lambda _ : \"\"\n",
    "# prefixE = r'\\textsc{gd-E}' if 'gd' in baseline.lower() else r'\\textsc{Saynt-E}' if 'saynt' in baseline.lower() else 'WHAT?'\n",
    "# prefixU = r'\\textsc{gd-U}' if 'gd' in baseline.lower() else r'\\textsc{Saynt-U}' if 'saynt' in baseline.lower() else 'WHAT?'\n",
    "ax.set_yticklabels([\"\\\\textsc{Saynt} $M_{\" + f\"{i+1}\" + \"}$\" for i in range(subfamily_size)] + (['\\\\textsc{gd-U}', '\\\\textsc{Saynt-U}'] if union_results else []) + ['\\\\textsc{rfPG-S}']) # [\"rfPG on subfamily\"])\n",
    "# ax.set_yticklabels([prefixE + f\" on POMDP {i+1}{suffix(i)}\" for i in range(subfamily_size)] + ([prefixU + f\" on Union POMDP\"] if union_results else []) + [r'\\textsc{rfPG-S} on Subfamily']) # [\"rfPG on subfamily\"])\n",
    "# ax.set_title(f\"{title}: {baseline} baselines vs \" + r'\\textsc{rfPG-S}' + f\" for a single run ({'lower' if minimizing else 'higher'} is better)\")\n",
    "# ax.set_title(get_env_str(env).replace('3','5'))\n",
    "plt.tight_layout()\n",
    "print(env)\n",
    "plt.savefig(f\"{OUTPUT_DIR}/heatmaps/{env}-main-body.{save_format}\", **save_args)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\",font_scale=2)\n",
    "for env, minimizing in zip(ENV_NAMES, MINIMIZING):\n",
    "    # if '8-3' not in env:\n",
    "        # continue\n",
    "    # try:\n",
    "    create_heatmap(env, minimizing=minimizing, include_union_results=True, plot_gradient_baseline=True, seed=SEEDS[0])\n",
    "    # break\n",
    "    # except:\n",
    "        # print(env, 'failed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gather statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pomdp_families import POMDPFamiliesSynthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_number_of_states(env):\n",
    "    gd = POMDPFamiliesSynthesis(env)\n",
    "    highest_nr_states = 0\n",
    "    for hole_combination in gd.pomdp_sketch.family.all_combinations():\n",
    "        assignment = gd.pomdp_sketch.family.construct_assignment(hole_combination)\n",
    "        pomdp = gd.pomdp_sketch.build_pomdp(assignment)\n",
    "        curr_nr_states = pomdp.model.nr_states\n",
    "        highest_nr_states = max(highest_nr_states, curr_nr_states)\n",
    "    return highest_nr_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_statistics_and_random_policy_value(env, stratified=True, subfamily_size=SUBFAMILY_SIZE):\n",
    "    gd = POMDPFamiliesSynthesis(env)\n",
    "    family_size = gd.pomdp_sketch.family.size\n",
    "    rand_fsc = gd.random_fsc(1)\n",
    "    \n",
    "    dtmc_sketch = gd.get_dtmc_sketch(rand_fsc)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for seed in SEEDS:\n",
    "        if stratified:\n",
    "            subfamily_assigments, hole_combinations = gd.stratified_subfamily_sampling(subfamily_size, seed=seed)\n",
    "        else:\n",
    "            subfamily_assigments, hole_combinations = gd.create_random_subfamily(subfamily_size)\n",
    "    \n",
    "        evaluations = gd.get_values_on_subfamily(dtmc_sketch, subfamily_assigments)\n",
    "        \n",
    "        results[seed] = {\n",
    "            'hole_combinations' : hole_combinations,\n",
    "            'evaluations' : evaluations\n",
    "        }\n",
    "\n",
    "    _, family_value = gd.paynt_call(dtmc_sketch)\n",
    "    \n",
    "    results['family_value'] = family_value\n",
    "    \n",
    "    results['family_size'] = gd.pomdp_sketch.family.size\n",
    "    results['num_actions'] = gd.pomdp_sketch.num_actions\n",
    "    results['num_observations'] = gd.pomdp_sketch.num_observations\n",
    "    \n",
    "    results['max_num_states'] = get_max_number_of_states(env)\n",
    "    \n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# statistics = {}\n",
    "# for env in ENVS:\n",
    "#     statistics[env] = get_statistics_and_random_policy_value(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"./statistics.pickle\", 'wb') as handle:\n",
    "#     pickle.dump(statistics, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
